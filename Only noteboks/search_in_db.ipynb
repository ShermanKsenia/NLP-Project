{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CGu90xiBw2k6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LaKrMU3sw63u"
   },
   "outputs": [],
   "source": [
    "tags = ('A', 'ADV', 'ADVPRO', 'ANUM', 'APRO', 'COM', 'CONJ', 'INTJ', 'NUM', 'PART', 'PR', 'S', 'SPRO', 'V', 'PRT', 'GRND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(tag):\n",
    "    new_tags = {'ADV': 'ADVB', 'ADVPRO': 'ADVB', 'ANUM': 'NUMR', 'APRO': 'ADJ', 'NUM': 'NUMR',\n",
    "                'PART': 'PRCL', 'PR': 'PREP', 'S': 'NOUN', 'SPRO': 'NPRO', 'V': 'VERB', 'A': 'ADJ'}\n",
    "    if tag in new_tags:\n",
    "        new_tag = new_tags[tag]\n",
    "        return new_tag\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y6HyDLhcw7n6"
   },
   "outputs": [],
   "source": [
    "def lemmatization(word, tag=None):\n",
    "    w = morph.parse(word)\n",
    "    l = w[0].normal_form\n",
    "    if tag:\n",
    "        tag = standardize(tag)\n",
    "        for ana in w:\n",
    "            if tag == 'ADJ' or tag == 'PRT':\n",
    "                if ana.tag.POS.startswith(tag):\n",
    "                    l = ana.normal_form\n",
    "                    break\n",
    "            else:\n",
    "                if ana.tag.POS == tag:\n",
    "                    l = ana.normal_form\n",
    "                    break\n",
    "    return (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ARM7rj3nw9ol"
   },
   "outputs": [],
   "source": [
    "def only_one_word(phrase):\n",
    "    to_search = {'token': None, 'lemma': None, 'pos': None}\n",
    "    if phrase[0].upper() in tags:\n",
    "        phrase = phrase[0].upper()\n",
    "        to_search['pos'] = phrase\n",
    "        #print('ищем в столбце pos')\n",
    "    else:\n",
    "        phrase = phrase[0].lower()\n",
    "        if re.search(r'[a-z]', phrase):\n",
    "            print('Латинские символы могут быть использованы только для частеречных тегов')\n",
    "            return None\n",
    "        elif phrase[0] == '\"':\n",
    "            to_search['token'] = phrase[1:-1]\n",
    "            #print('ищем в столбце token')\n",
    "        else:\n",
    "            l = lemmatization(phrase)\n",
    "            to_search['lemma'] = l\n",
    "    return to_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jNyEzuVhxBCa"
   },
   "outputs": [],
   "source": [
    "def token_and_tag(phrase):\n",
    "    if re.search(r'[a-z]', phrase[0].lower()):\n",
    "        print('Формат введенного запроса неверный')\n",
    "        return None\n",
    "    else:\n",
    "        to_search = {'token': None, 'lemma': None, 'pos': None}\n",
    "        w = phrase[0].lower() #слово\n",
    "        tag = phrase[1] #тег\n",
    "        if w[0] == '\"':\n",
    "            to_search['token'] = w[1:-1]\n",
    "        else:\n",
    "            l = lemmatization(w, tag)\n",
    "            to_search['lemma'] = l\n",
    "        p = phrase[1].upper()\n",
    "        if p not in tags:\n",
    "            print('Указан неправильный частеречный тег')\n",
    "            return None\n",
    "        else:\n",
    "            to_search['pos'] = p\n",
    "            return to_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ER4sYvsYxEAP"
   },
   "outputs": [],
   "source": [
    "def main_search(q):\n",
    "    if re.search(r'[^a-zA-Zа-яА-Я +\"]', q):\n",
    "        print('В запросе присутствуют некорректные символы')\n",
    "        return None\n",
    "    else:\n",
    "        request = []\n",
    "        q = q.split()\n",
    "        if len(q) > 3:\n",
    "            print('Превышена длина запроса')\n",
    "        elif len(q) == 1:    # значит в запросе одно слово\n",
    "            q1 = q[0].split('+')\n",
    "            if len(q1) == len(q):    # либо только токен, либо только тег\n",
    "                result = only_one_word(q1)\n",
    "                if result is not None:\n",
    "                    request.append(result)\n",
    "            else:    # в запросе токен и тег\n",
    "                result = token_and_tag(q1)\n",
    "                if result is not None:\n",
    "                    request.append(result)\n",
    "        else:    # в запросе не одно слово\n",
    "            for element in q:\n",
    "                q1 = element.split('+')\n",
    "                if len(q1) == 1:    # либо только токен, либо только тег\n",
    "                    result = only_one_word(q1)\n",
    "                    if result is not None:\n",
    "                        request.append(result)\n",
    "                    else:\n",
    "                        request = []\n",
    "                        break\n",
    "                else:    # в запросе токен и тег\n",
    "                    result = token_and_tag(q1)\n",
    "                    if result is not None:\n",
    "                        request.append(result)\n",
    "                    else:\n",
    "                        request = []\n",
    "                        break\n",
    "    return request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h-TQTTcqxLHh"
   },
   "outputs": [],
   "source": [
    "q = 'мыла+V'    # сюда введите запрос\n",
    "q_list = main_search(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ee392gJE_Se",
    "outputId": "c11e0fd3-e6e9-4a93-8305-46b8a0db01b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': None, 'lemma': 'мыть', 'pos': 'V'}]\n"
     ]
    }
   ],
   "source": [
    "print(q_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hpIMO12ETQs7"
   },
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_4IlhfbiTW85"
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('poems_corpus.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R0iWA7Cbo3M5"
   },
   "outputs": [],
   "source": [
    "def token_and_pos(dct):\n",
    "    query = \"\"\"\n",
    "    SELECT token_id, sent_id\n",
    "    FROM tokens\n",
    "    WHERE token = ? AND pos = ?\n",
    "    \"\"\"\n",
    "    cur.execute(query, (dct['token'], dct['pos']))\n",
    "    result = cur.fetchall()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "o2Wg0EcOvDZq"
   },
   "outputs": [],
   "source": [
    "def lemma_and_pos(dct):\n",
    "    query = \"\"\"\n",
    "    SELECT token_id, sent_id\n",
    "    FROM tokens\n",
    "    WHERE lemma = ? AND pos = ?\n",
    "    \"\"\"\n",
    "    cur.execute(query, (dct['lemma'], dct['pos']))\n",
    "    result = cur.fetchall()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pYwAlR1awMOV"
   },
   "outputs": [],
   "source": [
    "def only_token(dct):\n",
    "    query = \"\"\"\n",
    "    SELECT token_id, sent_id\n",
    "    FROM tokens\n",
    "    WHERE token = ?\n",
    "    \"\"\"\n",
    "    cur.execute(query, (dct['token'],))\n",
    "    result = cur.fetchall()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ziummee9wMX_"
   },
   "outputs": [],
   "source": [
    "def only_lemma(dct):\n",
    "    query = \"\"\"\n",
    "    SELECT token_id, sent_id\n",
    "    FROM tokens\n",
    "    WHERE lemma = ?\n",
    "    \"\"\"\n",
    "    cur.execute(query, (dct['lemma'],))\n",
    "    result = cur.fetchall()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Iw-RUABdRpvI"
   },
   "outputs": [],
   "source": [
    "def only_pos(dct):\n",
    "    query = \"\"\"\n",
    "    SELECT token_id, sent_id\n",
    "    FROM tokens\n",
    "    WHERE pos = ?\n",
    "    \"\"\"\n",
    "    cur.execute(query, (dct['pos'],))\n",
    "    result = cur.fetchall()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HcG2frmQnxPg"
   },
   "outputs": [],
   "source": [
    "def get_for_next(result, next):\n",
    "    sentences = []\n",
    "    if next['token']:\n",
    "        item = 'token'\n",
    "        if next['pos']:\n",
    "            item += ' = ? AND pos'\n",
    "    elif next['lemma']:\n",
    "        item = 'lemma'\n",
    "        if next['pos']:\n",
    "            item += ' = ? AND pos'\n",
    "    elif next['pos']:\n",
    "        item = 'pos'\n",
    "    t_query = f\"\"\"\n",
    "    SELECT token_id, sent_id\n",
    "    FROM tokens\n",
    "    WHERE {item} = ? AND token_id = ? AND sent_id = ?\n",
    "    \"\"\"\n",
    "    for token_id, sent_id in result:\n",
    "        next_id = token_id + 1\n",
    "        if ' ' in item:\n",
    "            items = item.split(' = ? AND ')\n",
    "            cur.execute(t_query, (next[items[0]], next[items[1]], next_id, sent_id))\n",
    "        else:\n",
    "            cur.execute(t_query, (next[item], next_id, sent_id))\n",
    "        cur_sent = cur.fetchone()\n",
    "        if cur_sent:\n",
    "            sentences.append(cur_sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vctYIu21577y"
   },
   "outputs": [],
   "source": [
    "def sort_sentences(q_list):\n",
    "    if q_list[0]['token'] and q_list[0]['pos']: # если в словарь токена записывать не пустые строки, а None\n",
    "        result = token_and_pos(q_list[0]) # id токенов и предложений\n",
    "    elif q_list[0]['lemma'] and q_list[0]['pos']:\n",
    "        result = lemma_and_pos(q_list[0])\n",
    "    elif q_list[0]['token'] and not q_list[0]['pos']:\n",
    "        result = only_token(q_list[0])\n",
    "    elif q_list[0]['lemma'] and not q_list[0]['pos']:\n",
    "        result = only_lemma(q_list[0])\n",
    "    else:\n",
    "        result = only_pos(q_list[0])\n",
    "    if result and len(q_list) > 1:\n",
    "        sentences = get_for_next(result, q_list[1])\n",
    "        if len(q_list) == 3:\n",
    "            sentences = get_for_next(sentences, q_list[2])\n",
    "            sentences = [obj[1] for obj in sentences]\n",
    "            return sentences\n",
    "        else:\n",
    "            sentences = [obj[1] for obj in sentences]\n",
    "            return sentences # получаем список айдишников предложений\n",
    "    if result:\n",
    "        result = [obj[1] for obj in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DtluGvbxZT_",
    "outputId": "785193f7-a9b1-474d-bc80-16dcc136b878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1264, 1832, 2809, 2890, 4591, 5081, 8143]\n"
     ]
    }
   ],
   "source": [
    "print(sort_sentences(q_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = sort_sentences(q_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(ids):\n",
    "    if len(ids) == 1:\n",
    "        ids = f'({ids[0]})'\n",
    "    else:\n",
    "        ids = tuple(ids)\n",
    "    posts_query = f'''\n",
    "        SELECT poet, title, dirty_sent FROM sentences\n",
    "        JOIN poems_to_info ON sentences.id_sent== poems_to_info.id_sent\n",
    "        JOIN info ON poems_to_info.id_info == info.id_info\n",
    "        WHERE sentences.id_sent in {ids}\n",
    "    '''\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(posts_query)\n",
    "    result = cur.fetchall()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Владимир Маяковский', 'Гуляем', 'Раз шесть\\nмоет лапкой\\nна морде шерсть.'),\n",
       " ('Владимир Маяковский',\n",
       "  'История Власа, лентяя и лоботряса',\n",
       "  'Разошлись\\nдругие\\nв школы, —\\nВлас\\nу крана\\nполуголый —\\nне дремалось в школе чтоб,\\nмоет нос\\nи мочит лоб.'),\n",
       " ('Александр Блок',\n",
       "  'На поле Куликовом',\n",
       "  'Течет, грустит лениво\\nИ моет берега.'),\n",
       " ('Сергей Есенин',\n",
       "  'Не бродить, не мять в кустах багряных',\n",
       "  'В тихий час, когда заря на крыше,\\nКак котенок, моет лапкой рот,\\nГовор кроткий о тебе я слышу\\nВодяных поющих с ветром сот.'),\n",
       " ('Владимир Маяковский',\n",
       "  'Про это',\n",
       "  'Человек из-за 7-ми лет\\nВолны устои стальные моют.'),\n",
       " ('Владимир Маяковский',\n",
       "  'Про это',\n",
       "  'Что хотите, буду делать даром —\\nчистить,\\nмыть,\\nстеречь,\\nмотаться,\\nместь.'),\n",
       " ('Владимир Маяковский',\n",
       "  'Что такое хорошо и что такое плохо',\n",
       "  'Этот\\nчистит валенки,\\nмоет\\nсам\\nгалоши.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentences(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
